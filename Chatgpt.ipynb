{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rladphone/AI/blob/main/Chatgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##step0) Colab í™˜ê²½ ì„¤ì •\n",
        "ì„¤ì¹˜(python>=3.8)"
      ],
      "metadata": {
        "id": "wsxhb4RzJ7MQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj7Xp2LBJ3cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "776949f7-e69a-4e52-d5c4-04705a153bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: colossalai==0.2.7 in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (23.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (8.1.3)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (3.0.1)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (1.11.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from colossalai==0.2.7) (2.0.1+cu118)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai==0.2.7) (3.1.0)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (1.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (6.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai==0.2.7) (20.23.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai==0.2.7) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->colossalai==0.2.7) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->colossalai==0.2.7) (16.0.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai==0.2.7) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai==0.2.7) (67.7.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai==0.2.7) (1.5.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai==0.2.7) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->colossalai==0.2.7) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->colossalai==0.2.7) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai==0.2.7) (2.21)\n",
            "/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/colossalai_ChatGPT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/colossalai_ChatGPT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.29.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (4.65.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.12.0)\n",
            "Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.1.1)\n",
            "Requirement already satisfied: colossalai>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from chatgpt==0.1.0) (0.0.113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (23.1)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (13.3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (8.1.3)\n",
            "Requirement already satisfied: fabric in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (3.0.1)\n",
            "Requirement already satisfied: contexttimer in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (0.3.3)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from colossalai>=0.2.4->chatgpt==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->chatgpt==0.1.0) (0.13.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets->chatgpt==0.1.0) (0.18.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.4.48)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (0.5.7)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->chatgpt==0.1.0) (8.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->chatgpt==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->chatgpt==0.1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->chatgpt==0.1.0) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->chatgpt==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.20.1->chatgpt==0.1.0) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain->chatgpt==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: invoke>=2.0 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.10/dist-packages (from fabric->colossalai>=0.2.4->chatgpt==0.1.0) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->chatgpt==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->chatgpt==0.1.0) (2022.7.1)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (2.5.24)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in /usr/local/lib/python3.10/dist-packages (from pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (20.23.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->colossalai>=0.2.4->chatgpt==0.1.0) (2.14.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->chatgpt==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->colossalai>=0.2.4->chatgpt==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nodeenv>=0.11.1->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (67.7.2)\n",
            "Requirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (4.0.1)\n",
            "Requirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (40.0.2)\n",
            "Requirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->chatgpt==0.1.0) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->chatgpt==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (0.3.6)\n",
            "Requirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->colossalai>=0.2.4->chatgpt==0.1.0) (3.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric->colossalai>=0.2.4->chatgpt==0.1.0) (2.21)\n",
            "Building wheels for collected packages: chatgpt\n",
            "  Building wheel for chatgpt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chatgpt: filename=chatgpt-0.1.0-py3-none-any.whl size=46647 sha256=4e8ee5b192b0b00e2c4074ac838d667dad172829012bdf7b5526ed29cac91543\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_p_w4agk/wheels/1a/46/ee/b32b27bbd787ce7100af8f0dfbeab0c4034eead588d448e162\n",
            "Successfully built chatgpt\n",
            "Installing collected packages: chatgpt\n",
            "  Attempting uninstall: chatgpt\n",
            "    Found existing installation: chatgpt 0.1.0\n",
            "    Uninstalling chatgpt-0.1.0:\n",
            "      Successfully uninstalled chatgpt-0.1.0\n",
            "Successfully installed chatgpt-0.1.0\n",
            "/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain==0.0.113 in /usr/local/lib/python3.10/dist-packages (0.0.113)\n",
            "Requirement already satisfied: PyYAML<7,>=6 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.4.48)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (3.8.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (0.5.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.113) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.113) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain==0.0.113) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.113) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2,>=1->langchain==0.0.113) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.113) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.29.2\n",
            "    Uninstalling transformers-4.29.2:\n",
            "      Successfully uninstalled transformers-4.29.2\n",
            "Successfully installed transformers-4.28.0\n"
          ]
        }
      ],
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1\n",
        "!pip install transformers==4.28.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1) SFT: ì§ˆë¬¸ì— ëŒ€ë‹µì„ ì˜í•˜ëŠ” ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "SFT(Supervised Fine Tuning)\n",
        "\n",
        "*   ì§ˆë¬¸ì— ì‘ë‹µì„ ì˜í•˜ë„ë¡ SFT ìˆ˜í–‰\n",
        "*   ì‚¬ëŒì´ ì§€ì‹œí•œ ëŒ€ë‹µ(13,000ê°œ), ì§ˆë¬¸ì— ì‘ë‹µ(13,000ê°œ)\n",
        "* ì§ˆë¬¸-ì‘ë‹µì˜ ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° ì…‹\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fti52YUFK_Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, pipeline\n",
        "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
        "from copy import deepcopy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import copy\n",
        "import logging\n",
        "import json\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)"
      ],
      "metadata": {
        "id": "NDrVciWgJ-Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path_1_SFT', type=str, default='/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/data_kochatgpt/kochatgpt_1_SFT_custom.jsonl')\n",
        "parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--max_epochs', type=int, default=2)\n",
        "parser.add_argument('--train_batch_size', type=int, default=8)\n",
        "parser.add_argument('--output_dir', type=str, default='./output_1_SFT')\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.model_name = 'skt/kogpt2-base-v2'  # SK GPT2, https://github.com/SKT-AI/KoGPT2\n",
        "args.max_epochs = 2\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hLeeD7wLa--",
        "outputId": "6c210ad3-da21-4928-a185-bf81f4c1c124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data_path_1_SFT='/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/data_kochatgpt/kochatgpt_1_SFT_custom.jsonl', model_name='skt/kogpt2-base-v2', max_epochs=2, train_batch_size=8, output_dir='./output_1_SFT')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## test & load skt gpt2 kroean\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "                                                    pad_token='<pad>', mask_token='<mask>')\n",
        "print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\"))\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "gen_ids = model.generate(input_ids,\n",
        "                         max_length=128,\n",
        "                         repetition_penalty=2.0,\n",
        "                         pad_token_id=tokenizer.pad_token_id,\n",
        "                         eos_token_id=tokenizer.eos_token_id,\n",
        "                         bos_token_id=tokenizer.bos_token_id,\n",
        "                         use_cache=True)\n",
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "bLM29OoULp_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b12da3-4c5d-400c-e9cb-d41aeb041c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['â–ì•ˆë…•', 'í•˜', 'ì„¸', 'ìš”.', 'â–í•œêµ­ì–´', 'â–G', 'P', 'T', '-2', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ˜¤', ':)', 'l^o']\n",
            "ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?\"\n",
            "\"ê·¸ë ‡ë‹¤ë©´ ê·¸ê±´ ë°”ë¡œ 'ë‚´ê²Œ ë§ëŠ”' ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ë°©ë²•ì€ ì•„ì£¼ ê°„ë‹¨í•©ë‹ˆë‹¤. ìš°ì„ , ë¨¼ì € ë‚´ ëª¸ì— ìˆëŠ” ì§€ë°©ì„ ì œê±°í•´ ì¤ë‹ˆë‹¤. ì§€ë°©ì„¸í¬ëŠ” ìš°ë¦¬ ëª¸ì—ì„œ ê°€ì¥ ë§ì´ ë¶„í¬ë¼ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì§€ë°©ì´ ë§ì€ ì‚¬ëŒì€ ìì‹ ì˜ ëª¸ ì†ì— ë“¤ì–´ìˆëŠ” ì§€ë°©ì˜ ì–‘ì„ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ë‚˜ëŠ” ì´ê²ƒì„ í†µí•´ ì²´ì¤‘ì„ ê°ëŸ‰í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ ì‰½ê²Œ ì‚´ì„ ë¹¼ëŠ” ê²ƒì€ ì¢‹ì§€ ì•Šìœ¼ë‹ˆ ì£¼ì˜í•´ì•¼ê² ì£ . ì™œëƒí•˜ë©´ ì§€ë°©ì€ ëŒ€ë¶€ë¶„ ê·¼ìœ¡ê³¼ ì¸ëŒ€ ë“±ì— ë¶„í¬ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ê·¸ë§Œí¼ì˜ ì–‘ì´ ë§ê¸° ë•Œë¬¸ì´ì§€ìš”. ê·¸ëŸ¬ë¯€ë¡œ ë‹¤ì´ì–´íŠ¸ë¥¼ í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì „ë¬¸ì˜ì™€ ìƒì˜í•´ì•¼ í•˜ê² ì§€ìš”?\n",
            "ì´ë ‡ê²Œ í•˜ë©´ ì‚´ì´ ì°Œê¸° ì „ì—\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "                                                    pad_token='<pad>', mask_token='<mask>')\n",
        "print(len(tokenizer.tokenize(\"í•˜ëŠ˜ì€ íŒŒë€ìƒ‰ ì…ë‹ˆë‹¤.\")))"
      ],
      "metadata": {
        "id": "YUPS5KClwHTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1057bc62-f260-4fad-b9a6-9c71f1879ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    eos_token_id=375,\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "generator(\n",
        "    [\"0 : AIì— ê´€ì‹¬ì´ ìˆë‹ˆ?\\n:\",\n",
        "    \"0 : AIë¼ëŠ” ê²ƒì€ ë„ˆë¬´ ì¬ë¯¸ìˆëŠ”ê±° ê°™ì•„\\n: ë§ì•„ AIëŠ” ë„ˆë¬´ í¥ë¯¸ë¡œì›Œ \\n: ë„ˆë„ ê·¸ë ‡ê²Œ ëŠë¼ë‹ˆ? ë§ì•„ AIë¼ëŠ”ê²ƒì€ ë„ˆë¬´ ë‹¤ì–‘í•˜ê³  ì‹ ê¸°í•´\\n:\"],\n",
        "    **generation_args\n",
        ")"
      ],
      "metadata": {
        "id": "_GSTaz1ZHMFd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eec0c56-b1cf-4843-f1d6-527872d89e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'generated_text': '0 : AIì— ê´€ì‹¬ì´ ìˆë‹ˆ?\\n: ê´œì°®ì•„.\\n: ì•„, ê·¸ë ‡êµ¬ë‚˜.\\n: ê·¸ë˜.\\n: ë„¤ê°€ ì•Œê³  ìˆëŠ” ê²ƒ ê°™ì€ë°.\\n: ì˜ˆ, ì•Œê² ìŠµë‹ˆë‹¤.\\n: ë„¤. ì–´ë¨¸ë‹˜ê»˜ ë§ì”€ë“œë¦´ê²Œìš”.\\n: ë„¤. ì œê°€ ì•Œê³  ìˆëŠ” ê²Œ ë­ì˜ˆìš”?\\n: ê·¸ëŸ¼ìš”.\\n: ì•„ë‹ˆì—ìš”.\\n: ê·¸ê±´ ì¢€ ì´ìƒí•˜ë„¤'}],\n",
              " [{'generated_text': '0 : AIë¼ëŠ” ê²ƒì€ ë„ˆë¬´ ì¬ë¯¸ìˆëŠ”ê±° ê°™ì•„\\n: ë§ì•„ AIëŠ” ë„ˆë¬´ í¥ë¯¸ë¡œì›Œ \\n: ë„ˆë„ ê·¸ë ‡ê²Œ ëŠë¼ë‹ˆ? ë§ì•„ AIë¼ëŠ”ê²ƒì€ ë„ˆë¬´ ë‹¤ì–‘í•˜ê³  ì‹ ê¸°í•´\\n: ë‚˜ë„ AIë¼ê³  í•  ìˆ˜ ìˆì–´\\n'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n###Input(ì…ë ¥):\\n{input}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "h1yApLcWLquf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ëª¨ë¸ ì¤€ë¹„\n",
        "model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    args.model_name,\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,    \n",
        ")\n",
        "tokenizer.add_special_tokens(\n",
        "    {\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    }\n",
        ")    \n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "0wi0Q5PPL838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59c6d5a7-1035-4d8e-9c71-00cd771051e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## prepare data\n",
        "from typing import Optional, Dict, Sequence\n",
        "    \n",
        "class SFT_dataset(Dataset):\n",
        "    '''SFT dataset by wygo'''\n",
        "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
        "        super(SFT_dataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        \n",
        "        ## format\n",
        "        pattern_instruction = 'prompt'  # instruction\n",
        "        pattern_input = 'input'  \n",
        "        pattern_output = 'completion'  \n",
        "\n",
        "        ############################################################\n",
        "        ## load dataset\n",
        "\n",
        "        data_path_1_SFT = '/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/data_kochatgpt/kochatgpt_1_SFT_custom.jsonl'\n",
        "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
        "            list_data_dict = json.load(json_file)\n",
        "            if verbose:\n",
        "                print('## data check ##')\n",
        "                print((list_data_dict[0]))\n",
        "\n",
        "        ############################################################\n",
        "        ## ë°ì´í„°ì…‹ ë§Œë“¤ê¸°, sourceì™€ target\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]  # í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°\n",
        "\n",
        "        # ì…ë ¥\n",
        "        sources = []\n",
        "        for example in list_data_dict:\n",
        "            if example.get(pattern_input, \"\") != \"\":\n",
        "                tmp = prompt_input.format_map(example)\n",
        "            else:\n",
        "                tmp = prompt_no_input.format_map(example)\n",
        "            sources.append(tmp)\n",
        "\n",
        "        # ì¶œë ¥\n",
        "        targets = []\n",
        "        for example in list_data_dict:\n",
        "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
        "\n",
        "        if verbose:\n",
        "            idx = 0\n",
        "            print((sources[idx]))\n",
        "            print((targets[idx]))\n",
        "            print(\"Tokenizing inputs... This may take some time...\")\n",
        "\n",
        "        ############################################################\n",
        "        examples = [s + t for s, t in zip(sources, targets)]\n",
        "\n",
        "        # source data tokenized\n",
        "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # sourceë§Œ\n",
        "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
        "\n",
        "\n",
        "        ## ì…ë ¥ì€ source, ì¶œë ¥ì€ source+target ì´ì§€ë§Œ í•™ìŠµì€ target ë¶€ë¶„ë§Œ\n",
        "        input_ids = examples_tokenized[\"input_ids\"]\n",
        "        labels = copy.deepcopy(input_ids)\n",
        "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "            label[:source_len] = IGNORE_INDEX  # source ë¶€ë¶„ì€ -100ìœ¼ë¡œ ì±„ìš´ë‹¤\n",
        "        \n",
        "        data_dict = dict(input_ids=input_ids, labels=labels)        \n",
        "        \n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))    \n",
        "        \n",
        "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "        \"\"\"Tokenize a list of strings.\"\"\"\n",
        "        tokenized_list = [\n",
        "            tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"longest\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "            )\n",
        "            for text in strings\n",
        "        ]\n",
        "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "        input_ids_lens = labels_lens = [\n",
        "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "        ]\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            input_ids_lens=input_ids_lens,\n",
        "            labels_lens=labels_lens,\n",
        "        )        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "    \n",
        "\n",
        "train_dataset = SFT_dataset(data_path_1_SFT=args.data_path_1_SFT, tokenizer=tokenizer)\n",
        "eval_dataset  = None  # evalì€ ì•ˆí•¨\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "# check\n",
        "print('input : %s'%train_dataset.input_ids[0])\n",
        "print('output: %s'%train_dataset.labels[0])"
      ],
      "metadata": {
        "id": "EHAVJPAbL_F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa015b9c-642a-4fb6-d5b9-ff2c4084b660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Loading data done!!: 12002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input : tensor([14659, 13394, 37091, 10651,   383, 25841,  8006, 14914,  7673, 20479,\n",
            "         8091, 22311,  9036, 30902, 13675,   375,   424,  9792,   454,  9549,\n",
            "        20549,   383,  8142,  7192, 14914,   382, 37767, 13753,  8263,  7166,\n",
            "          739,  8352,  7659,  9594, 25585, 13600,  8022,  9378, 11532,  9887,\n",
            "        11218,  9111, 16691, 10351, 10561,  9128, 20479,  8091,  9065,  9446,\n",
            "         9036, 28420, 26521, 10163, 26367,  6958,  9030,  9882, 12317, 25882,\n",
            "         9209, 37194, 10351,  9036, 12168, 10529, 15989,  9719, 15434, 10552,\n",
            "        11188, 13362,  9036, 15805, 11300, 11846,  9146, 16691,  9181,  7397,\n",
            "        15806, 13480, 11342, 17596,  9161, 19996,  9025, 25006, 18595,  9966,\n",
            "        12592, 10751, 11814,  8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
            "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
            "         -100,  -100,  -100,  -100,  -100,   382, 37767, 13753,  8263,  7166,\n",
            "          739,  8352,  7659,  9594, 25585, 13600,  8022,  9378, 11532,  9887,\n",
            "        11218,  9111, 16691, 10351, 10561,  9128, 20479,  8091,  9065,  9446,\n",
            "         9036, 28420, 26521, 10163, 26367,  6958,  9030,  9882, 12317, 25882,\n",
            "         9209, 37194, 10351,  9036, 12168, 10529, 15989,  9719, 15434, 10552,\n",
            "        11188, 13362,  9036, 15805, 11300, 11846,  9146, 16691,  9181,  7397,\n",
            "        15806, 13480, 11342, 17596,  9161, 19996,  9025, 25006, 18595,  9966,\n",
            "        12592, 10751, 11814,  8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3wy8oGVgi_4",
        "outputId": "580ef79b-3be6-4dab-b2d2-91811bfdceed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## í•™ìŠµ (10min)\n",
        "# training_args ìˆ˜ì • ê°€ëŠ¥: https://github.com/Beomi/KoAlpaca/blob/main/train.sh ì°¸ê³ \n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=1, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 3, # Number of update steps between two evaluations.\n",
        "    save_steps=500, # after # steps model is saved \n",
        "    warmup_steps=5,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    )\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_state()\n",
        "safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)"
      ],
      "metadata": {
        "id": "Vi1iZ5VXMo9v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "d929565b-99b1-4c84-f888-ed1e3426308e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3001' max='3001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3001/3001 13:04, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.113300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.952300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.849600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.786300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.704300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.664700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPT2ëª¨ë¸ì´ ì‚¬ëŒì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëŒ€ë‹µí•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•¨"
      ],
      "metadata": {
        "id": "EPct4AxNM_gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)\n",
        "# generator = pipeline('text-generation', model=model.cpu(), tokenizer=tokenizer, config={'max_length':800})\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0, #ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì–µì œí•¨\n",
        "    no_repeat_ngram_size=4, #ê°œì˜ ì—°ì†ì ì¸ ë‹¨ì–´ë¥¼ ë°˜ë³µí•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
        "    eos_token_id=375, #EOS(End-of-Sequence)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í† í° IDë¥¼ ì§€ì • ë° í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ëª¨ë¸ì€ ì´ EOS í† í°ì— ë„ë‹¬í•˜ë©´ í† í° ìƒì„±ì„ ì¤‘ì§€\n",
        "    max_new_tokens=64,#ìƒì„±í•˜ëŠ” í† í°ì˜ ìµœëŒ€ ìˆ˜\n",
        "    do_sample=True, #Trueë¡œ ì„¤ì •í•˜ë©´ ëª¨ë¸ì´ ì˜ˆì¸¡ í™•ë¥ ì— ë”°ë¼ ë‹¤ìŒ í† í°ì„ ë¬´ì‘ìœ„ ì„ íƒ \n",
        "    top_k=50, #ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í• ë•Œ ì˜ˆì¸¡ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ìƒìœ„ k ê°œë¡œ ì˜ˆì¸¡í•¨\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "list_prompt = [\"AIë€ ë¬´ì—‡ì¸ê°€?\", \"ê°•ì›ë„ëŠ” ë¬´ì—‡ì´ ìœ ëª…í•œê°€ìš”?\"]\n",
        "list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
        "\n",
        "list_result = generator(list_prompt, **generation_args)\n",
        "for prompt, result in zip(list_prompt, list_result):\n",
        "    print((\"-----------------Chat AIë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.-----------------\\n\"))\n",
        "    print(('ì§ˆë¬¸ : {}\\n\\n').format(result[0]['generated_text']))"
      ],
      "metadata": {
        "id": "-_Ni7ztKM2Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2) RM : ì¢‹ì€ ê¸€ ì±„ì ê¸° ë§Œë“¤ê¸°\n",
        "\n",
        "* ìƒì„±ëœ ê° ê¸€ì— ëŒ€í•œ ì ìˆ˜ë¥¼ ë§¤ê¸°ê³  ì´ë¥¼ ë³´ìƒëª¨ë¸ë¡œ ë§Œë“¬"
      ],
      "metadata": {
        "id": "RLGRyTPpNMQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "!pip install torch==1.13.1\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1"
      ],
      "metadata": {
        "id": "G9-DqeHDg42G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4yQwLBnBFof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "import loralib as lora\n",
        "import torch\n",
        "from chatgpt.dataset import RewardDataset\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.models.bloom import BLOOMRM\n",
        "from chatgpt.models.gpt import GPTRM\n",
        "from chatgpt.models.opt import OPTRM\n",
        "from chatgpt.trainer import RewardModelTrainer\n",
        "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
        "from datasets import load_dataset\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from colossalai.nn.optimizer import HybridAdam\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\n Response(ì‘ë‹µ):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n Response(ì‘ë‹µ):\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "7mZJmQZiNQ2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--output_dir', type=str, default='./output_2_RM')\n",
        "parser.add_argument('--data_path_2_RM', type=str, default='/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/data_kochatgpt/kochatgpt_2_RM_custom.jsonl')\n",
        "parser.add_argument('--strategy',\n",
        "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
        "                    default='naive')\n",
        "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--dataset', type=str, default='Dahoas/rm-static')\n",
        "parser.add_argument('--save_path', type=str, default='rm_ckpt.pth')\n",
        "parser.add_argument('--max_epochs', type=int, default=10)\n",
        "parser.add_argument('--batch_size', type=int, default=4)\n",
        "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument('--max_len', type=int, default=512)  # wygo ì¶”ê°€\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.max_epochs = 3\n",
        "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "args.verbose = True\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ],
      "metadata": {
        "id": "YlYLhC-7NmuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure strategy\n",
        "if args.strategy == 'naive':\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == 'ddp':\n",
        "    strategy = DDPStrategy()\n",
        "elif args.strategy == 'colossalai_gemini':\n",
        "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
        "elif args.strategy == 'colossalai_zero2':\n",
        "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
      ],
      "metadata": {
        "id": "4C7fHvYGOGg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# customizing, https://github.com/hpcaitech/ColossalAI/blob/2e16f842a9e5b1fb54e7e41070e9d2bb5cd64d7c/applications/ChatGPT/chatgpt/nn/gpt_rm.py#L29\n",
        "from typing import Optional\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "# from ..base import RewardModel\n",
        "from chatgpt.models.base import RewardModel\n",
        "\n",
        "\n",
        "class GPTRM_custom(RewardModel):\n",
        "    \"\"\"\n",
        "    GPT Reward model.\n",
        "    Args:\n",
        "        pretrained (str): Pretrained model name or path.\n",
        "        config (GPT2Config): Model config.\n",
        "        checkpoint (bool): Enable gradient checkpointing.\n",
        "        lora_rank (int): Rank of the low-rank approximation.\n",
        "        lora_train_bias (str): LoRA bias training mode.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 pretrained: Optional[str] = None,\n",
        "                 config: Optional[GPT2Config] = None,\n",
        "                 checkpoint: bool = False,\n",
        "                 lora_rank: int = 0,\n",
        "                 lora_train_bias: str = 'none',\n",
        "                 tokenizer=None) -> None:\n",
        "        if pretrained is not None:\n",
        "            model = GPT2Model.from_pretrained(pretrained)\n",
        "            model.resize_token_embeddings(len(tokenizer))  # wygo ì¶”ê°€!!!\n",
        "        elif config is not None:\n",
        "            model = GPT2Model(config)\n",
        "        else:\n",
        "            model = GPT2Model(GPT2Config())\n",
        "        if checkpoint:\n",
        "            model.gradient_checkpointing_enable()\n",
        "\n",
        "        # model = model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        value_head = nn.Linear(model.config.n_embd, 1)\n",
        "        super().__init__(model, value_head, lora_rank, lora_train_bias)"
      ],
      "metadata": {
        "id": "YgSF1K2uOLNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure model, tokenizer\n",
        "with strategy.model_init_context():\n",
        "    # load pretrained gpt2    \n",
        "    if args.model == 'gpt2':\n",
        "#         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(args.pretrain)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model = GPTRM_custom(pretrained=args.pretrain, lora_rank=args.lora_rank, tokenizer=tokenizer).cuda()\n",
        "\n",
        "    elif args.model == 'bloom':\n",
        "        model = BLOOMRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
        "    \n",
        "    elif args.model == 'opt':\n",
        "        model = OPTRM(pretrained=args.pretrain, lora_rank=args.lora_rank).cuda()\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")      \n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
        "    \n",
        "    \n",
        "    # model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "ab2NqplAOPOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make ranking data to chosen, rejetced data\n",
        "with open(args.data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    if args.verbose:\n",
        "        print('## data check ##')\n",
        "        print((list_data_dict[0]))\n",
        "        \n",
        "total_data_ranking2chosen = []\n",
        "for tmp in list_data_dict:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # data 1) 0 VS 1\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "\n",
        "    # data 2) 0 VS 2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # data 1) 1 VS 2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "    \n",
        "    \n",
        "    \n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "print('before data num: %d'%(len(list_data_dict)))\n",
        "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
        "print('data example: \\n%s'%total_data_ranking2chosen[45])"
      ],
      "metadata": {
        "id": "lr2hPwPFOS6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_data_ranking2chosen)"
      ],
      "metadata": {
        "id": "85ca93LKB5xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare for data and dataset\n",
        "import random\n",
        "random.seed(230319)\n",
        "# list_tmp = list(range(10))\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "print(total_data_ranking2chosen[45])\n",
        "\n",
        "# train_data = total_data_ranking2chosen[:-1000]  # 29000 í•™ìŠµ\n",
        "# eval_data = total_data_ranking2chosen[-1000:0]  # 1000ê°œë§Œ í‰ê°€\n",
        "\n",
        "train_data = total_data_ranking2chosen[:100]  # í•™ìŠµ ë°ì´í„°ì˜ ìˆ˜\n",
        "eval_data = total_data_ranking2chosen[100:130]  # í‰ê°€ ë°ì´í„°ì˜ ìˆ˜\n",
        "\n",
        "train_dataset = RewardDataset(train_data, tokenizer, args.max_len)\n",
        "eval_dataset = RewardDataset(eval_data, tokenizer, args.max_len)\n",
        "\n",
        "# check\n",
        "idx = 10\n",
        "print('#'*70)\n",
        "print('## prompt ##')\n",
        "print(train_data[idx]['prompt'])\n",
        "print('#'*70)\n",
        "print('## chosen ##')\n",
        "print(train_data[idx]['chosen'])\n",
        "print('#'*70)\n",
        "print('## rejected ##')\n",
        "print(train_data[idx]['rejected'])"
      ],
      "metadata": {
        "id": "kv358luNOX8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure optimizer\n",
        "if args.strategy.startswith('colossalai'):\n",
        "    optim = HybridAdam(model.parameters(), lr=5e-5)\n",
        "else:\n",
        "    optim = Adam(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "rq99mcy2OjIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size here is expected to be C(k,2), k means # response of each prompt\n",
        "# be limited with the format of dataset 'Dahoas/rm-static', we'd better use batch_size as 1\n",
        "trainer = RewardModelTrainer(model=model,\n",
        "                             strategy=strategy,\n",
        "                             optim=optim,\n",
        "                             train_dataset=train_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=args.batch_size,\n",
        "                             max_epochs=args.max_epochs)"
      ],
      "metadata": {
        "id": "2Ua2vKRgOkjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RMì„ í†µí•´ ê° ìˆœìœ„ì— ëŒ€í•´ ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  ê°€ì¥ ì •í™•í•œ ì‘ë‹µì— ëŒ€í•´ í•™ìŠµí•¨"
      ],
      "metadata": {
        "id": "m25-9pd7Oq3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train!!\n",
        "trainer.fit(use_lora=args.lora_rank)\n",
        "\n",
        "## save\n",
        "# save model checkpoint after fitting on only rank0\n",
        "strategy.save_model(model, os.path.join(args.output_dir, 'RM.pt'), only_rank0=True)\n",
        "# save optimizer checkpoint on all ranks\n",
        "strategy.save_optimizer(optim,\n",
        "                        os.path.join(args.output_dir, 'RM_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ],
      "metadata": {
        "id": "m5DeYx6jOmWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3) PPO ì‚¬ëŒì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì´ í”¼ë“œë°±ê³¼ ë¹„ìŠ·í•´ ì§€ë„ë¡ í•™ìŠµ"
      ],
      "metadata": {
        "id": "-vFmdP46P3cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## setup(1min)\n",
        "# for ColossalAI\n",
        "!pip install colossalai==0.2.7\n",
        "!pip install torch==1.13.1\n",
        "# setup data\n",
        "\n",
        "%cd /content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/colossalai_ChatGPT/\n",
        "!pip install .\n",
        "%cd ../../\n",
        "\n",
        "# setup library\n",
        "!pip install openai\n",
        "!pip install langchain==0.0.113\n",
        "!pip install pandas>=1.4.1"
      ],
      "metadata": {
        "id": "l_vRtPXodlmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.models.bloom import BLOOMActor, BLOOMCritic\n",
        "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
        "from chatgpt.models.opt import OPTActor, OPTCritic\n",
        "from chatgpt.trainer import PPOTrainer\n",
        "from chatgpt.trainer.strategies import ColossalAIStrategy, DDPStrategy, NaiveStrategy\n",
        "from torch.optim import Adam\n",
        "from transformers import AutoTokenizer, BloomTokenizerFast\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "from colossalai.nn.optimizer import HybridAdam\n",
        "\n",
        "## wy ì¶”ê°€\n",
        "import json\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "## clossalAI error í•´ê²°\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['LOCAL_RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '2'\n",
        "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "os.environ['MASTER_PORT'] = '42043'\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "RTa3nVDuO5Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_path_3_PPO', type=str, default='/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/GPT/data_kochatgpt/kochatgpt_3_PPO.jsonl')\n",
        "parser.add_argument('--output_dir', type=str, default='./output_3_PPO')\n",
        "parser.add_argument('--strategy',\n",
        "                    choices=['naive', 'ddp', 'colossalai_gemini', 'colossalai_zero2'],\n",
        "                    default='naive')\n",
        "parser.add_argument('--model', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--num_episodes', type=int, default=10)\n",
        "parser.add_argument('--max_timesteps', type=int, default=3)\n",
        "parser.add_argument('--update_timesteps', type=int, default=3)\n",
        "parser.add_argument('--max_epochs', type=int, default=5)\n",
        "parser.add_argument('--train_batch_size', type=int, default=8)\n",
        "parser.add_argument('--lora_rank', type=int, default=0, help=\"low-rank adaptation matrices rank\")\n",
        "parser.add_argument('--max_length', type=int, default=250)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# for test\n",
        "args.output_dir = './output_3_PPO'\n",
        "args.pretrain = 'skt/kogpt2-base-v2'  # pretrained ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "\n",
        "# args.pretrain_actor = './output_1_SFT'  # SFT ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "# args.pretrain_critic = './output_2_RM'  # RM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°\n",
        "args.pretrain_actor = args.pretrain\n",
        "args.pretrain_critic = args.pretrain\n",
        "\n",
        "args.num_episodes = 1\n",
        "args.max_epochs   = 1\n",
        "\n",
        "print(args)\n",
        "if not os.path.exists(args.output_dir):\n",
        "    os.makedirs(args.output_dir)"
      ],
      "metadata": {
        "id": "LzLnJEHZPB2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.pretrain"
      ],
      "metadata": {
        "id": "Pisr96oIccRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure strategy\n",
        "if args.strategy == 'naive':\n",
        "    strategy = NaiveStrategy()\n",
        "elif args.strategy == 'ddp':\n",
        "    strategy = DDPStrategy()\n",
        "elif args.strategy == 'colossalai_gemini':\n",
        "    strategy = ColossalAIStrategy(stage=3, placement_policy='cuda')\n",
        "elif args.strategy == 'colossalai_zero2':\n",
        "    strategy = ColossalAIStrategy(stage=2, placement_policy='cuda')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported strategy \"{args.strategy}\"')"
      ],
      "metadata": {
        "id": "a9xRGXwiPNs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure model, tokenizer\n",
        "with strategy.model_init_context():\n",
        "    if args.model == 'gpt2':\n",
        "        actor = GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = GPTCritic(pretrained=args.pretrain_critic, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        # tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.pretrain, padding_side=\"right\", model_max_length=512)\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )    \n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "\n",
        "    elif args.model == 'bloom':\n",
        "        actor = BLOOMActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = BLOOMCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = BloomTokenizerFast.from_pretrained(args.pretrain)\n",
        "        tokenizer.pad_token = tokenizer.eos_token            \n",
        "    elif args.model == 'opt':\n",
        "        actor = OPTActor(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        critic = OPTCritic(pretrained=args.pretrain, lora_rank=args.lora_rank).to(torch.cuda.current_device())\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")            \n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model \"{args.model}\"')\n",
        "\n",
        "    initial_model = deepcopy(actor)\n",
        "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
      ],
      "metadata": {
        "id": "Og7ueBVvPPtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure optimizer\n",
        "if args.strategy.startswith('colossalai'):\n",
        "    actor_optim = HybridAdam(actor.parameters(), lr=5e-6)\n",
        "    critic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n",
        "else:\n",
        "    actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
        "    critic_optim = Adam(critic.parameters(), lr=5e-6)"
      ],
      "metadata": {
        "id": "9DhMVvYjPR2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the models\n",
        "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n",
        "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
      ],
      "metadata": {
        "id": "Sp_dgLebPTe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data\n",
        "with open(args.data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
        "\n",
        "def tokenize_fn(texts):\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
        "    return {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "print(list_prompt)\n",
        "print('\\n\\n\\n')\n",
        "print(tokenize_fn('I want you to act as a linux terminal.'))"
      ],
      "metadata": {
        "id": "ThJWdu2aPVD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure trainer\n",
        "trainer = PPOTrainer(strategy,\n",
        "                     actor,\n",
        "                     critic,\n",
        "                     reward_model,\n",
        "                     initial_model,\n",
        "                     actor_optim,\n",
        "                     critic_optim,\n",
        "                     max_epochs=args.max_epochs,\n",
        "                     train_batch_size=args.train_batch_size,\n",
        "                     tokenizer=tokenize_fn,\n",
        "                     max_length=128,\n",
        "                     do_sample=True,\n",
        "                     temperature=1.0,\n",
        "                     top_k=50,\n",
        "                     pad_token_id=tokenizer.pad_token_id,\n",
        "                     eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "## train!\n",
        "trainer.fit(list_prompt,  # ì…ë ¥ prompt\n",
        "            num_episodes=args.num_episodes,\n",
        "            max_timesteps=args.max_timesteps,\n",
        "            update_timesteps=args.update_timesteps)\n",
        "\n",
        "## save\n",
        "# save model checkpoint after fitting on only rank0\n",
        "strategy.save_model(actor, os.path.join(args.output_dir, 'actor.pt'), only_rank0=True)\n",
        "# save optimizer checkpoint on all ranks\n",
        "strategy.save_optimizer(actor_optim,\n",
        "                        os.path.join(args.output_dir, 'actor_optim_checkpoint_%d.pt' % (torch.cuda.current_device())),\n",
        "                        only_rank0=False)"
      ],
      "metadata": {
        "id": "mctryMQGPV6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ydF3RbQE0Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor=GPTActor(pretrained=args.pretrain_actor, lora_rank=args.lora_rank)"
      ],
      "metadata": {
        "id": "qXGgCwz5KH01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## inference\n",
        "def generation(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(\"-\"*70)\n",
        "    print((\"ì±—ë´‡ : {}\").format(output))\n",
        "    print(\"-\"*70)\n",
        "    return output"
      ],
      "metadata": {
        "id": "1ahSVzSxPZdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_prompt = [\"ì¸ê³µì§€ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ ì„¤ëª…í•´ì¤˜\",\"í•˜ëŠ˜ì—ëŠ” ì™œ êµ¬ë¦„ì´ ìˆì„ê¹Œ?\"]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "  print(\"-\"*70)\n",
        "  print((\"ì§ˆë¬¸ : {}\").format(input_text))\n",
        "  output = generation(input_text)"
      ],
      "metadata": {
        "id": "2ldehpLuJEk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QA-sQ0ig5M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h16qOhQug5J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##í•™ìŠµ ë°ì´í„° í™œìš©í•˜ê¸°"
      ],
      "metadata": {
        "id": "vdUS_YHlhXeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from chatgpt.models.bloom import BLOOMActor\n",
        "from chatgpt.models.gpt import GPTActor\n",
        "from chatgpt.models.opt import OPTActor\n",
        "from transformers import AutoTokenizer\n",
        "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
        "\n",
        "# data config\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\n### Input(ì…ë ¥):\\n{input}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Instruction(ëª…ë ¹ì–´):{prompt}\\n\\nResponse(ì‘ë‹µ):\"\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "XgXMqdh5g5Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# define argment\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model',\n",
        "                    default='gpt2',\n",
        "                    choices=['gpt2', 'bloom', 'opt'])\n",
        "# We suggest to use the pretrained model from HuggingFace, use pretrain to configure model\n",
        "parser.add_argument('--pretrain', type=str, default=None)\n",
        "parser.add_argument('--model_path', type=str, default=None)\n",
        "parser.add_argument('--input',\n",
        "                    type=str,\n",
        "                    default='Question: How are you ? Answer:')\n",
        "parser.add_argument('--max_length', type=int, default=250)\n",
        "args_inference = parser.parse_args([])\n",
        "\n",
        "args_inference.model = 'gpt2'\n",
        "args_inference.pretrain = 'skt/kogpt2-base-v2'\n",
        "args_inference.model_directory = '/content/drive/MyDrive/ì¸ê³µì§€ëŠ¥ê°•ì˜/output_3_PPO'\n",
        "args_inference.model_path = os.path.join(args_inference.model_directory, 'actor.pt')\n",
        "\n",
        "# configure model, tokenizer\n",
        "if args_inference.model == 'gpt2':\n",
        "    #actor = GPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
        "    actor = GPTActor(pretrained=args_inference.pretrain)\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained(args_inference.pretrain)\n",
        "    # tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args_inference.pretrain,\n",
        "                                              padding_side=\"right\",\n",
        "                                              model_max_length=512)\n",
        "    tokenizer.add_special_tokens({\n",
        "        \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "        \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "        \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "    })\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "elif args_inference.model == 'bloom':\n",
        "    actor = BLOOMActor(pretrained=args_inference.pretrain).to(\n",
        "        torch.cuda.current_device())\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "elif args_inference.model == 'opt':\n",
        "    actor = OPTActor(pretrained=args_inference.pretrain).to(torch.cuda.current_device())\n",
        "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
        "else:\n",
        "    raise ValueError(f'Unsupported model \"{args_inference.model}\"')\n",
        "\n",
        "state_dict = torch.load(args_inference.model_path, map_location='cpu');\n",
        "actor.model.load_state_dict(state_dict);\n",
        "\n",
        "actor.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynju55ZIg7_t",
        "outputId": "cf77da63-1b18-4b82-a1c9-16a625a8b64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## inference\n",
        "def generation(input_text):\n",
        "    #input_ids = tokenizer.encode(input_text, return_tensors='pt').to(torch.cuda.current_device())\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "    outputs = actor.generate(input_ids,\n",
        "                             max_length=args_inference.max_length,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
        "    print(\"-\"*70)\n",
        "    print((\"ì±—ë´‡ : {}\").format(output))\n",
        "    print(\"-\"*70)\n",
        "    return output"
      ],
      "metadata": {
        "id": "1XGTiOOJhLaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_prompt = [\"ì¸ê³µì§€ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ ì„¤ëª…í•´ì¤˜\",\"í•˜ëŠ˜ì—ëŠ” ì™œ êµ¬ë¦„ì´ ìˆì„ê¹Œ?\"]\n",
        "\n",
        "for input_text in list_prompt:\n",
        "  print(\"-\"*70)\n",
        "  print((\"ì§ˆë¬¸ : {}\").format(input_text))\n",
        "  output = generation(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P4Qp1hqhNPW",
        "outputId": "2f0fbabf-82b3-4db2-d6fb-bf49f10bfebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "ì§ˆë¬¸ : ì¸ê³µì§€ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ ì„¤ëª…í•´ì¤˜\n",
            "----------------------------------------------------------------------\n",
            "ì±—ë´‡ : ì¸ê³µì§€ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ ì„¤ëª…í•´ì¤˜ìš”.\n",
            "ì–´ë¨¸ë‹˜ì´ ë³´ì‹œëŠ” ê²Œ ë°”ë¡œ ê·¸ê±°ì˜ˆìš”?\n",
            "ì–´ë¨¸ë‹˜ì€ ê·¸ê²Œ ë­”ì§€ ì•„ì„¸ìš”?\n",
            "ì•„ë‹ˆ ê·¸ëƒ¥ ê·¸~ ì•Œ ìˆ˜ê°€ ì—†ëŠ” ê±´ë° ì™œ ê·¸~ ê·¸~ ê·¸~ ë´‡ë¬¼ í„°ì§€ëŠ” ê·¸~ ì˜ìƒì„ ë³´ê³  ì§„ì§œ ì–´ë¨¸ë‹˜ì€ ì–¼ë§ˆë‚˜ ì´ì˜ê²Œ ë³´ì…¨ì„ê¹Œë¼ê³  ìƒê°í•˜ì‹œê² ì§€ë§Œ ê·¸~ ê·¸~ ê·¸~ ê·¸~ ì–´ë¨¸ë‹˜ì´ ì´ê²Œ ë¬´ìŠ¨ ì†Œë¦´ í•˜ì‹œëŠ”ì§€ ëª°ëì§€ë§Œ ì´ì œ ê·¸~ ë­ë„ê¹Œ ë˜ê²Œ ê¶ê¸ˆí•˜ì–ì•„ìš”.\n",
            "ê·¸~ ê·¸~ ê·¸~ ì•„~ ê·¸~ ê·¸~ ê·¸~ ê·¸~ ë‹ˆê¹Œ?\n",
            "ì•„~ ê·¸ë˜ì„œ ê·¸~ ì•„~ ì œê°€ ê·¸~ ì§€ê¸ˆ ê·¸~ ë‹ˆê¹Œ?\n",
            "ê·¸ë˜ì„œ ê·¸~ ê·¸~ ë‹ˆê¹Œ?\n",
            "ê·¸~ ì•„~ ì•„ë‹ˆ ê·¼ë° ì•„ë‹ˆ ê·¸~ ë‹ˆê¹Œ?\n",
            "ê·¸~ ê·¸~ ì•„~ ê·¸ëŸ¼ ì–´~ ë­ì•¼ ì•„~ ê·¸~ ì•„~ ì–´~ ì´~ ê·¸~ ë­”ì§€ ê¶ê¸ˆí•˜ì–ì•„ìš”.\n",
            "ê·¸~ ë‹ˆ?\n",
            "ê·¸~ ê·¸~ ë‹ˆê¹Œ?\n",
            "ê·¸~ ì•„~ ë­”ì§€ ê¶ê¸ˆí•œ ê±°ì˜ˆìš”?\n",
            "ì•„ë‹ˆ ë‹ˆê¹Œ?\n",
            "ë‹ˆê¹Œ?\n",
            "ì•„ë‹ˆ ë­ì•¼ ê·¸ë˜ì„œ ë‹ˆê¹Œ?\n",
            "ì•„ë‹ˆ ê·¸~ ì €í¬ ì•„~ ê·¸~\n",
            "----------------------------------------------------------------------\n",
            "----------------------------------------------------------------------\n",
            "ì§ˆë¬¸ : í•˜ëŠ˜ì—ëŠ” ì™œ êµ¬ë¦„ì´ ìˆì„ê¹Œ?\n",
            "----------------------------------------------------------------------\n",
            "ì±—ë´‡ : í•˜ëŠ˜ì—ëŠ” ì™œ êµ¬ë¦„ì´ ìˆì„ê¹Œ? ì–´ë•Œ? ì§€ê¸ˆì´ ë•¡ë•¡ì´ì´ì•¼. ìš°ë¦¬ ì§‘ì´ ë‹¤ ì¢‹ì•˜ì§€.\"\n",
            "ê·¸ ë§ì— ë†€ë€ ì—„ë§ˆëŠ” ë§í–ˆë‹¤.\n",
            "\"ë‚˜ëŠ” ë‹¤ë“¤ ì¢‹ì•˜ì–´. ë‚œ ì—„ë§ˆí•œí…Œ ì˜ëì–´. ë„ˆí¬ë“¤ ë‹¤ ì˜ëì–´. ë‚´ ì•„ë“¤ë„ ì—„ë§ˆê°€ ì˜ëìœ¼ë©´ ì¢‹ê² ë‹¤ê³ . ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ì´ ì˜ë¼ì„œ ì¢‹ì•˜ì–´.\"\n",
            "\"ê·¸ë ‡ë‹¤ë©´ ì´ì œ ì§‘ìœ¼ë¡œ ëŒì•„ê°€ë„ ë¼?\"\n",
            "\"ë‚´ê°€ ì—„ë§ˆí•œí…Œ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ê·¸ëŸ¼, ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ì—ê²Œ ì˜ë  ê±°ì•¼. ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ì€ ë‹¤ ì˜ëì–´. ë‚´ ì•„ë“¤ë„ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ì´ë ‡ê²Œ ë§í–ˆê±°ë“ . ë‚´ê°€ ì—„ë§ˆí•œí…Œ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´. ë‚´ ì•„ë“¤ë„ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ì—„ë§ˆ. ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ ë‹¤ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´. ì—„ë§ˆëŠ” ë•¡ë•¡ì´ë„ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ê·¸ë˜. ìš°ë¦¬ ì§‘ ì‚¬ëŒë“¤ì—ê²Œ ì˜ëì–´. ìš°ë¦¬ ë™ë„¤ ì‚¬ëŒë“¤ì€ ë‹¤ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ê·¸ëŸ¼, ì—„ë§ˆ. ì´ê²Œ ì—„ë§ˆê°€ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´. ì—„ë§ˆê°€ ë•¡ë•¡ì´ë¥¼ ë§ì´ ì˜í–ˆìœ¼ë©´ ì¢‹ê² ì–´. ì—„ë§ˆëŠ” ë‹¤ ì˜ëìœ¼ë©´ ì¢‹ê² ì–´.\"\n",
            "\"ê·¸ëŸ¼ ë‚´ê°€ ë„ˆ\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sKFjohc4OGLk"
      }
    }
  ]
}